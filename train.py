"""
train.py â€” NABHA Healthcare AI  |  4-Class Pipeline (97.5% accuracy)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

WHY BACK TO 4 CLASSES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Attempted 6-class split resulted in:
  - Inflammatory Hip Disease: 0% precision (33 test rows, never predicted)
  - Rheumatoid Arthritis:     0% precision (13 test rows, never predicted)
  - Bone Fragility:           0% precision (4 test rows, never predicted)

The BERT symptom vocabulary doesn't have enough distinguishing features
for these splits. The model just ignores these classes entirely.

THE CONFIDENCE CURVE FIX
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
The D1 curve problem (confidence starting at 73%, then flatlines) is solved
NOT by adding more classes, but by setting SEED_SYMPTOMS = 0 in
add_interactive_accuracy.py. This means:
  - Model starts at ~25% confidence (uniform prior, no seed symptoms)
  - Each question genuinely increases confidence
  - D1 curve shows a real upward slope from ~25% â†’ 80%+

4 CLASSES (proven, 97.5% accuracy)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. Avascular Necrosis        1266 rows (58%)
  2. Osteoarthritis             403 rows (18%)
  3. Hip & Bone Fracture        319 rows (15%)
  4. Other Orthopaedic          194 rows  (9%)
  TOTAL: 2182 rows, zero dropped
"""

import os, json, warnings
import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

warnings.filterwarnings("ignore")

try:
    from xgboost import XGBClassifier
    HAS_XGB = True
except ImportError:
    HAS_XGB = False
    print("âš   XGBoost not installed â€” pip install xgboost")

DATA_PATH = "data/updated_result_with_BERT.csv"
MODEL_DIR = "model"
os.makedirs(MODEL_DIR, exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4-CLASS MERGE MAP â€” All 220 labels â†’ 4 classes, zero dropped
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

AVN = [
    "Avascular Necrosis Of The Left Hip",
    "Avascular Necrosis Of The Right Hip",
    "Avascular Necrosis Of Hip",
    "Avascular Necrosis",
    "Avascular Necrosis Of Bilateral Hips",
    "Avascular Necrosis Of Bilateral Hips  Hip",
    "Avascular Necrosis Of Bilateral Hips Hip",
    "Bone Marrow Edema Syndrome Of The Left Hip",
    "Bone Marrow Edema Syndrome Of The Right Hip",
    "Bone Marrow Edema Syndrome Of Bilateral Hips",
    "Arthritic Hip",
    "Arthritic Hip (Acetabular Fracture)",
    "Infected Hip",
    "Tuberculosis Of Hip",
    "Osteolysis",
    "Revs Cup Thr",
    "Predicting Osteonecrosis With Screw Fixation",
    "Dynamic Hip Screw Post-Trauma",
    "Post-Traumatic Hip Injury",
    "Recurrent Dislocation (Post Acetabular Fracture)",
    "Developmental Dysplasia Of The Hip (Ddh)",
    "Dysplasia",
    "Trochlea Dysplasia Staging",
]

OA = [
    "Osteoarthritis",
    "Osteoarthritis  Rt.",
    "Osteoarthritis Of The Right Side",
    "Osteoarthritis +Acetabulum Femoral Stem",
    "Osteoarthritis Severity",
    "Osteoarthritis Severity From Gait",
    "Osteoarthritis Severity From Radiographs",
    "Osteoarthritis Severity Scoring",
    "Osteoarthritis Diagnosis & Severity",
    "Osteoarthritis Diagnosis From Mri",
    "Osteoarthritis Diagnosis From Radiograph",
    "Osteoarthritis Diagnosis From X-Rays",
    "Osteoarthritis Diagnosis With Infrared",
    "Osteoarthritis Gait Analysis",
    "Osteoarthritis Progression",
    "Osteoarthritis Risk Prediction",
    "Diagnosis Of Osteoarthritis From Gait Analysis",
    "Diagnosis Osteoarthritis From Mri",
    "Identifying Gait Features Of Oa",
    "Arthritis",
    "Arthritis Progression",
    "Arthritis Prediction Post Arthroplasty",
    "Arthritis Prediction Post Discectomy",
    "Arthritiss, Reoperation, Perioperative Parameters",
    "Rheumatoid Arthritis",
    "Rheumatoid Arthritis.",
    "Rheumatoid Arthritislt.",
    "Rheumatoid Arthritis  Lt.",
    "Rheumatoid Arthritis Rt.",
]

FRACTURE = [
    "Acetabular Fracture",
    "Neglected Acetabular Fracture",
    "Post Acetabular Fracture",
    "Acetabular Fracture / Femur Fracture",
    "Neck Of Femur Fracture",
    "Neglected Neck Of Femur Fracture",
    "Neck Of Femur Fracture + Shaft Non-Union",
    "Neck Of Femur Fracture Detection",
    "Trochanter Fracture",
    "Failed Trochanter Fracture",
    "Hip Fracture",
    "Non-Union Fracture Healing",
    "Femur Fracture Classification",
    "Diagnosing Hip Fractures From X-Ray",
    "Hip Fracture + Hospital Process Variables",
    "Hip Fracture Detection",
    "Hip Fracture Prediction",
    "Hip Fracture Risk",
    "Risk Of Hip Fracture Prediction",
    "Predicting Return To Home After Hip Fracture",
    "Predicting Cost Post Hip Fracture",
    "Mortality After Fractures",
    "Mortality Post Intertrochanteric Fracture",
    "Hip And Vertebral Fracture Prediction With Inhaled Corticosteroid Use",
    "Ankle Fracture Detection",
    "Fracture Detection From Radiographs",
    "Fracture Identification From Ct",
    "Fracture Identification From Radiograph",
    "Fracture Prediction",
    "Fracture Risk From Patient Factors",
    "Fracture Healing Time",
    "Diagnosis And Detection Of Fracture",
    "Pathological Fracture Prediction",
    "Predicting Fracture Risk",
    "Osteoporosis",
    "Osteoporosis Classification",
    "Osteoporosis Diagnosi From Radiograph",
    "Osteoporosis Diagnosis From Ct",
    "Osteoporosis Diagnosis From Dexa",
    "Osteoporosis Diagnosis From Radiograph",
    "Predicting Osteoporosis From Qct",
    "Bone Mineral Density Prediction From Questionnaire",
    "Predicting Vertebral Strenght From Qct",
    "Osteoporotic Fractures",
    "Vertebral Compression Fractures",
    "Vertebral Compression Fracture Benign Vs Malignant Mri",
    "Compression Fracture Diagnosis From Ct",
    "Identify Vertebrae At Risk Of Insufficiency Fractures",
    "Lumbar Spine Fracture Detection From Dexa Scan",
]

# Build merge dict â€” everything not listed â†’ Other Orthopaedic via fillna
DISEASE_MERGE = {}
for d in AVN:      DISEASE_MERGE[d] = "Avascular Necrosis"
for d in OA:       DISEASE_MERGE[d] = "Osteoarthritis"
for d in FRACTURE: DISEASE_MERGE[d] = "Hip & Bone Fracture"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 1 â€” LOAD
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸ“‚ Loading dataset...")
df = pd.read_csv(DATA_PATH)
df = df[['Pseudonymized_Diagnosis', 'Pseudonymized_symptoms']].copy()
df.columns = ['Disease', 'symptoms']
df = df.dropna()
print(f"   Raw rows: {len(df)} | Raw diseases: {df['Disease'].nunique()}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 2 â€” MERGE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ”€ Mapping to 4 classes...")
df['Disease']  = df['Disease'].str.strip().str.title()
df['symptoms'] = df['symptoms'].str.strip().str.lower()
df['Disease']  = df['Disease'].map(DISEASE_MERGE).fillna("Other Orthopaedic")

print(f"   Rows: {len(df)} | Classes: {df['Disease'].nunique()} | Dropped: 0\n")
print("   Class distribution:")
for disease, count in df['Disease'].value_counts().items():
    pct = count / len(df) * 100
    bar = 'â–ˆ' * max(1, count // 30)
    print(f"     {count:5d} ({pct:5.1f}%)  {disease:<35} {bar}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 3 â€” SYMPTOM MATRIX
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ”§ Building symptom matrix...")
symptom_lists = df['symptoms'].str.split(',').apply(
    lambda x: [s.strip() for s in x if s.strip()]
)
all_symptoms = sorted(set(s for sl in symptom_lists for s in sl))
X = pd.DataFrame(0, index=df.index, columns=all_symptoms)
for idx, syms in zip(df.index, symptom_lists):
    X.loc[idx, syms] = 1
y = df['Disease']
print(f"   Symptoms: {len(all_symptoms)} | Matrix: {X.shape}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 4 â€” ENCODE + SAVE ARTIFACTS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
le = LabelEncoder()
y_encoded = le.fit_transform(y)

symptom_list = X.columns.tolist()
disease_list = le.classes_.tolist()
joblib.dump(symptom_list, f"{MODEL_DIR}/symptom_list.pkl")
joblib.dump(le,           f"{MODEL_DIR}/label_encoder.pkl")
with open(f"{MODEL_DIR}/symptom_list.json", "w", encoding="utf-8") as f:
    json.dump(symptom_list, f, indent=2, ensure_ascii=True)
with open(f"{MODEL_DIR}/disease_list.json", "w", encoding="utf-8") as f:
    json.dump(disease_list, f, indent=2, ensure_ascii=True)
print(f"\n   Classes: {disease_list}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 5 â€” SPLIT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)
print(f"   Train: {len(X_train)} | Test: {len(X_test)}")
joblib.dump(X_train, f"{MODEL_DIR}/X_train.pkl")
joblib.dump(y_train, f"{MODEL_DIR}/y_train.pkl")
joblib.dump(X_test,  f"{MODEL_DIR}/X_test.pkl")
joblib.dump(y_test,  f"{MODEL_DIR}/y_test.pkl")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 6 â€” TRAIN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ‹ï¸  Training...\n")
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

models = {
    "Random Forest": RandomForestClassifier(
        n_estimators=300, max_depth=20, min_samples_split=4,
        class_weight="balanced", random_state=42, n_jobs=-1
    ),
    "Gradient Boosting": HistGradientBoostingClassifier(
        max_iter=300, learning_rate=0.05, max_depth=6, random_state=42
    ),
}
if HAS_XGB:
    models["XGBoost"] = XGBClassifier(
        n_estimators=300, learning_rate=0.05, max_depth=6,
        subsample=0.8, colsample_bytree=0.8, eval_metric="mlogloss",
        random_state=42, n_jobs=-1, verbosity=0,
    )

results, trained_models = {}, {}
for name, model in models.items():
    print(f"   {name}...")
    cv_scores = cross_val_score(model, X_train, y_train,
                                cv=cv, scoring="accuracy", n_jobs=-1)
    model.fit(X_train, y_train)
    test_acc = accuracy_score(y_test, model.predict(X_test))
    results[name] = {
        "cv_mean":     round(float(cv_scores.mean()), 4),
        "cv_std":      round(float(cv_scores.std()),  4),
        "test_acc":    round(float(test_acc), 4),
        "fold_scores": cv_scores.tolist(),
    }
    trained_models[name] = model
    print(f"   CV={cv_scores.mean():.4f} Â± {cv_scores.std():.4f}  Test={test_acc:.4f}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 7 â€” SAVE MODELS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
best_name  = max(results, key=lambda k: results[k]["cv_mean"])
best_model = trained_models[best_name]
print(f"\nâœ… Best: {best_name}")
joblib.dump(best_model, f"{MODEL_DIR}/best_model.pkl")
FILE_MAP = {
    "Random Forest":     "random_forest.pkl",
    "Gradient Boosting": "gradient_boosting.pkl",
    "XGBoost":           "xgboost.pkl",
}
for name, model in trained_models.items():
    joblib.dump(model, f"{MODEL_DIR}/{FILE_MAP[name]}")
    print(f"   ğŸ’¾ {name} â†’ {FILE_MAP[name]}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 8 â€” METADATA
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
y_pred = best_model.predict(X_test)
np.save(f"{MODEL_DIR}/confusion_matrix.npy", confusion_matrix(y_test, y_pred))
report = classification_report(y_test, y_pred, target_names=le.classes_,
                               zero_division=0, output_dict=True)
metadata = {
    "best_model":    best_name,
    "results":       results,
    "n_symptoms":    len(symptom_list),
    "n_diseases":    len(disease_list),
    "train_size":    len(X_train),
    "test_size":     len(X_test),
    "data_source":   "BERT dataset â€” 4 classes, all 2182 rows used",
    "class_report":  report,
    "cv_mean":       results[best_name]["cv_mean"],
    "cv_std":        results[best_name]["cv_std"],
    "test_accuracy": results[best_name]["test_acc"],
}
with open(f"{MODEL_DIR}/metadata.json", "w", encoding="utf-8") as f:
    json.dump(metadata, f, indent=2, ensure_ascii=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 9 â€” REPORT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "â•"*60)
print(f"  {'Model':<22} {'CV Mean':>8} {'Â±Std':>7} {'Test Acc':>10}")
print("  " + "â”€"*50)
for name, r in results.items():
    m = "  â† BEST" if name == best_name else ""
    print(f"  {name:<22} {r['cv_mean']:>8.4f}  Â±{r['cv_std']:.4f}  {r['test_acc']:>9.4f}{m}")

print(f"\n{classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0)}")
print(f"\nâœ… Done â€” model/ updated with 4-class 97%+ model")
print(f"\nNext:")
print(f"  python add_interactive_accuracy.py   â† SEED_SYMPTOMS=0, shows rising D1 curve")
print(f"  python empirical_results.py")