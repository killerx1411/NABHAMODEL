"""
train_punjabi.py โ Punjabi training pipeline with 4-class disease merge
Maps all 27 Punjabi diseases โ 4 canonical groups matching the English model.

FINAL 4 CLASSES (same as English train.py)
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
  1. Avascular Necrosis    โ AVN variants, arthritic hip, BMES, DDH, infected hip, TB hip
  2. Osteoarthritis        โ OA + RA (indistinguishable by symptoms)
  3. Hip & Bone Fracture   โ all fracture variants
  4. Other Orthopaedic     โ spinal tumors, discharge/surgery outcomes, standalone AVN label

RUN
โโโ
  python train_punjabi.py
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer
from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import xgboost as xgb
import joblib
import os
import json
import unicodedata
import warnings
warnings.filterwarnings("ignore")

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# PATHS
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
DATA_PATH = "data/updated_result_with_AI_PUNJABI.csv"
MODEL_DIR = "model/punjabi"
os.makedirs(MODEL_DIR, exist_ok=True)

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# DISEASE MERGE MAP โ All 27 Punjabi labels โ 4 canonical classes
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

# โโ GROUP 1: AVASCULAR NECROSIS โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
AVASCULAR_NECROSIS_PA = [
    "เจเจฎเจฐ เจฆเฉ avascular necrosis",
    "เจฆเฉเจตเฉฑเจฒเฉ เจเฉเฉฑเจฒเฉเจนเฉ เจฆเจพ เจเจตเฉเจธเจเฉเจฒเจฐ เจจเฉเจเจฐเฉเจธเจฟเจธ",
    "เจเฉฑเจฌเฉ เจเจฎเจฐ เจฆเจพ เจเจตเฉเจธเจเฉเจฒเจฐ เจจเฉเจเจฐเฉเจธเจฟเจธ",
    "เจธเฉฑเจเฉ เจเจฎเจฐ เจฆเจพ เจเจตเฉเจธเจเฉเจฒเจฐ เจจเฉเจเจฐเฉเจธเจฟเจธ",
    "เจเจเฉเจ เจฆเฉ เจเจฎเจฐ",                              # arthritic hip
    "เจเจเฉเจ เจฆเฉ เจเจฎเจฐ (เจเจธเฉเจเฉเจฌเฉเจฒเจฐ เจซเฉเจฐเฉเจเจเจฐ)",
    "เจเฉฑเจฌเฉ เจเจฎเจฐ เจฆเจพ เจฌเฉเจจ เจฎเฉเจฐเฉ เจเจกเฉเจฎเจพ เจธเจฟเฉฐเจกเจฐเฉเจฎ",       # BMES left
    "เจธเฉฑPER_REPLACEMENT เจฆเจพ เจฌเฉเจจ เจฎเฉเจฐเฉ เจเจกเฉเจฎเจพ เจธเจฟเฉฐเจกเจฐเฉเจฎ", # BMES right (anonymised)
    "เจฆเฉเจตเฉฑเจฒเฉ เจเฉเฉฑเจฒเฉเจนเฉ เจฆเจพ เจฌเฉเจจ เจฎเฉเจฐเฉ เจเจกเฉเจฎเจพ เจธเจฟเฉฐเจกเจฐเฉเจฎ",   # BMES bilateral
    "เจกเจพเจเจจเจพเจฎเจฟเจ เจนเจฟเจช เจธเจเฉเจฐเฉ เจชเฉเจธเจ-เจเจฐเจพเจฎเจพ",             # Dynamic Hip Screw
    "เจเจฎเจฐ เจฆเฉ เจเฉ",                                 # TB hip (เจเฉ = TB)
    "เจธเฉฐเจเจฐเจฎเจฟเจค เจเจฎเจฐ",                               # infected hip
    "เจชเฉเจธเจ-เจเจฐเจพเจฎเฉเจเจฟเจ เจเจฎเจฐ เจฆเฉ เจธเฉฑเจ",                  # post-traumatic hip
    "ORG_REPLACEMENT เจธเฉฐเจฌเฉฐเจงเฉ เจกเจฟเจธเจชเจฒเฉเจธเฉเจ (เจกเฉเจกเฉเจเจ)",  # DDH (anonymised org name)
]

# โโ GROUP 2: OSTEOARTHRITIS โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# Note: Punjabi dataset has no OA X-ray diagnosis entry โ only 2 diseases here
OSTEOARTHRITIS_PA = [
    "เจเจเฉเจ",                  # Arthritis / OA
    "เจฐเจพเจเจฎเฉเจเจพเจเจก เจเจเฉเจ",        # Rheumatoid Arthritis
]

# โโ GROUP 3: HIP & BONE FRACTURE โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
HIP_BONE_FRACTURE_PA = [
    "acetabular เจซเฉเจฐเฉเจเจเจฐ",
    "acetabular fracture / femur fracture",
    "เจจเจเจผเจฐเจเฉฐเจฆเจพเจเจผ เจเจธเฉเจเจพเจฌเฉเจฒเจฐ เจซเฉเจฐเฉเจเจเจฐ",
    "เจเจฐเจฆเจจ เจฆเฉ เจซเจฐเฉเจเจเจฐ เจฆเฉ เจเจฐเจฆเจจ",                    # Neck of Femur Fracture
    "เจซเฉเจฎเจฐ เจซเฉเจฐเฉเจเจเจฐ เจฆเฉ เจเจฐเจฆเจจ + เจธเจผเจพเจซเจ เจเฉเจฐ-เจฏเฉเจจเฉเจเจจ",    # NOF + Shaft Non-Union
    "PER_REPLACEMENTเฉเจเจเจฐ เจฆเฉ เจเจฃเจฆเฉเจเฉ เจเจฐเจฆเจจ",         # Neglected NOF (anonymised)
    "trochanter เจซเฉเจฐเฉเจเจเจฐ",
    "เจเจธเจซเจฒ trochanter เจซเฉเจฐเฉเจเจเจฐ",
]

# โโ GROUP 4: OTHER ORTHOPAEDIC โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# Spinal + discharge + standalone AVN label (not hip-specific)
OTHER_ORTHOPAEDIC_PA = [
    "เจฐเฉเฉเฉเจน เจฆเฉ เจนเฉฑเจกเฉ เจฆเฉ เจเจฟเจเจฎเจฐ",           # Spinal Tumors
    "เจกORG_REPLACEMENT เจธเจฐเจเจฐเฉ",           # Discharge Destination Post Surgery (anonymised)
    "avascular necrosis",               # standalone English-label entry in Punjabi dataset
]

# Build merge dict
DISEASE_MERGE_PA = {}
for d in AVASCULAR_NECROSIS_PA:
    DISEASE_MERGE_PA[d] = "Avascular Necrosis"
for d in OSTEOARTHRITIS_PA:
    DISEASE_MERGE_PA[d] = "Osteoarthritis"
for d in HIP_BONE_FRACTURE_PA:
    DISEASE_MERGE_PA[d] = "Hip & Bone Fracture"
for d in OTHER_ORTHOPAEDIC_PA:
    DISEASE_MERGE_PA[d] = "Other Orthopaedic"

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# STEP 1 โ LOAD DATA
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
print("๐ Loading Punjabi medical dataset...")
df = pd.read_csv(DATA_PATH)

df = df[['Pseudonymized_Diagnosis', 'Pseudonymized_symptoms']].copy()
df.columns = ['Disease', 'symptoms']
df = df.dropna()
df['Disease']  = df['Disease'].str.strip()
df['symptoms'] = df['symptoms'].str.strip()

print(f"   Raw rows: {len(df)} | Raw diseases: {df['Disease'].nunique()}")

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# STEP 2 โ APPLY MERGE MAP
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
print("\n๐ Applying disease merge map...")

df['Disease'] = df['Disease'].map(DISEASE_MERGE_PA).fillna("Other Orthopaedic")

print(f"   After merge: {len(df)} rows | {df['Disease'].nunique()} canonical classes")
print(f"   Zero rows dropped โ all {len(df)} rows used\n")
print("   Class distribution:")
for disease, count in df['Disease'].value_counts().items():
    bar = 'โ' * (count // 5)
    print(f"     {count:5d}  {disease:<25} {bar}")

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# STEP 3 โ CLEAN & BUILD SYMPTOM MATRIX (Unicode safe, matching original)
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
print("\n๐ง Building symptom matrix...")

def normalize_text(text):
    return unicodedata.normalize("NFKC", text.strip())

symptom_lists = df["symptoms"].apply(
    lambda x: [normalize_text(s) for s in x.split(",") if s.strip()]
)

mlb = MultiLabelBinarizer()
X   = mlb.fit_transform(symptom_lists)
X   = pd.DataFrame(X, columns=mlb.classes_)

print(f"   Unique symptoms: {len(mlb.classes_)}")
print(f"   Matrix shape: {X.shape}")

y = df["Disease"].values

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# STEP 4 โ ENCODE LABELS
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
le        = LabelEncoder()
y_encoded = le.fit_transform(y)
print(f"   โ Encoded {len(le.classes_)} diseases")

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# STEP 5 โ SAVE ARTIFACTS
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
symptom_list = mlb.classes_.tolist()
disease_list = le.classes_.tolist()

joblib.dump(symptom_list, f"{MODEL_DIR}/symptom_list.pkl")
joblib.dump(le,           f"{MODEL_DIR}/label_encoder.pkl")

with open(f"{MODEL_DIR}/symptom_list.json", "w", encoding="utf-8") as f:
    json.dump(symptom_list, f, indent=2, ensure_ascii=False)
with open(f"{MODEL_DIR}/disease_list.json", "w", encoding="utf-8") as f:
    json.dump(disease_list, f, indent=2, ensure_ascii=False)

print(f"   โ Saved {len(symptom_list)} symptoms, {len(disease_list)} diseases")
print(f"   disease_list.json: {disease_list}")

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# STEP 6 โ TRAIN / TEST SPLIT
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)
print(f"\n๐ Train: {len(X_train)} | Test: {len(X_test)}")

# Convert to numpy for XGBoost stability
X_train_np = X_train.values
X_test_np  = X_test.values

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# STEP 7 โ TRAIN MODELS
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
print("\n๐๏ธ  Training models...\n")

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

models = {
    "Random Forest": RandomForestClassifier(
        n_estimators=200,
        max_depth=20,
        class_weight="balanced",
        random_state=42,
        n_jobs=-1
    ),
    "XGBoost": xgb.XGBClassifier(
        n_estimators=150,
        learning_rate=0.1,
        max_depth=6,
        eval_metric="mlogloss",
        random_state=42,
        n_jobs=-1,
        verbosity=0
    ),
    "Gradient Boosting": HistGradientBoostingClassifier(
        max_iter=200,
        learning_rate=0.05,
        max_depth=6,
        random_state=42
    ),
}

results        = {}
trained_models = {}

for name, model in models.items():
    print(f"   Training {name}...")
    cv_scores = cross_val_score(
        model, X_train_np, y_train, cv=cv, scoring="accuracy", n_jobs=-1
    )
    model.fit(X_train_np, y_train)
    test_acc = accuracy_score(y_test, model.predict(X_test_np))

    results[name] = {
        "cv_mean":     round(float(cv_scores.mean()), 4),
        "cv_std":      round(float(cv_scores.std()), 4),
        "test_acc":    round(float(test_acc), 4),
        "fold_scores": cv_scores.tolist(),
    }
    trained_models[name] = model
    print(f"   {name}: CV={cv_scores.mean():.4f} ยฑ {cv_scores.std():.4f} | Test={test_acc:.4f}")

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# STEP 8 โ SELECT & SAVE BEST MODEL
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
best_name  = max(results, key=lambda k: results[k]["cv_mean"])
best_model = trained_models[best_name]
print(f"\nโ Best model: {best_name}")

joblib.dump(best_model, f"{MODEL_DIR}/best_model.pkl")

y_pred = best_model.predict(X_test_np)
cm     = confusion_matrix(y_test, y_pred)
np.save(f"{MODEL_DIR}/confusion_matrix.npy", cm)

report = classification_report(
    y_test, y_pred,
    target_names=le.classes_,
    zero_division=0,
    output_dict=True
)

metadata = {
    "language":      "Punjabi",
    "best_model":    best_name,
    "results":       results,
    "n_symptoms":    len(symptom_list),
    "n_diseases":    len(disease_list),
    "train_size":    len(X_train),
    "test_size":     len(X_test),
    "data_source":   "Punjabi dataset โ 4 merged classes, all rows used",
    "class_report":  report,
    "cv_mean":       results[best_name]["cv_mean"],
    "cv_std":        results[best_name]["cv_std"],
    "test_accuracy": results[best_name]["test_acc"],
}

with open(f"{MODEL_DIR}/metadata.json", "w", encoding="utf-8") as f:
    json.dump(metadata, f, indent=2, ensure_ascii=False)

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# STEP 9 โ FINAL REPORT
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
print("\n๐ Classification Report:")
print(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0))
print("โ Done. Punjabi artifacts saved in /model/punjabi")
print(f"   disease_list.json: {disease_list}")